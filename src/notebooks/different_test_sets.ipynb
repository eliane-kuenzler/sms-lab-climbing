{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a606ab4-1133-4000-9ece-73c7f4ed7279",
   "metadata": {},
   "source": [
    "# Having different test sets: boulder and athlete specific"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fec8c0c-4b27-4e8a-8f6b-5d14608d8b20",
   "metadata": {},
   "source": [
    "**The goal is to have different test sets to simulate how the model would perform on a boulder it hasn't seen before and also how it would perform on athletes it hasn't seen before.**\n",
    "\n",
    "- Boulder specific test set: boulders W1, W2, W4 are used for training and boulder W3 for testing\n",
    "- Athlete specific test set: training set with 17 athletes per boulder and 4 athletes in the test set. The athletes in the test set are: \"Ai Mori\", \"Brooke Raboutou\", \"Oceania Mackenzie\" and \"Mia Krampl\". There are 4 different nations and also a wide variability in athlete size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0fceb2-9eb4-4c7c-b839-6d7deeb00dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b145b5d-4fe0-40b0-b7b1-c9f35e48aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/dataframes/labels_and_coordinates_preprocessed.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71dec4a-085c-439a-aa71-25fb1e77192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982faa1-8923-4187-beed-5c2d75574286",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['camera'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc3875-5280-4b1f-8899-334ec2d4cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['participant'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df262627-e75f-4402-9005-01af68167243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['boulder'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b61e6-0594-4554-94d5-f7b5e862c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['repetition'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acfab4b-09dc-4256-883a-004e675bcd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in the dataframe\n",
    "nan_counts = data.isna().sum()\n",
    "\n",
    "# sum of all NaN values in each column\n",
    "print(\"NaN values in each column:\")\n",
    "print(nan_counts)\n",
    "\n",
    "# total number of NaN values in the entire dataframe\n",
    "total_nan_values = data.isna().sum().sum()\n",
    "print(f\"\\nTotal number of NaN values in the dataframe: {total_nan_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2562357-ca2e-40f7-8f77-d35012be62b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTotal number of frames before dropping NaN values: {data.shape[0]}\")\n",
    "\n",
    "# dropping all the colums with NaN values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# how many frames are left\n",
    "print(f\"\\nTotal number of frames after dropping NaN values: {data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c9ab07-3bcc-46d0-a999-21dc66bc22e7",
   "metadata": {},
   "source": [
    "## 1 - Classic ML - Using boulder specific data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b4eb6-ef6b-4ec5-b7ff-a4ce67d854e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- some pre-processing -----------------\n",
    "# Define the manual mappings for each categorical column -> boulder, camera, participant, repetition\n",
    "boulder_mapping = {'W1': 1, 'W2': 2, 'W3': 3, 'W4': 4}\n",
    "\n",
    "camera_mapping = {'Cam21': 21, 'Cam22': 22, 'Cam24': 24}\n",
    "\n",
    "participant_mapping = {'Ai Mori': 1, 'Anastasia Sanders': 2, 'Ayala Kerem': 3, 'Brooke Raboutou': 4,\n",
    "                       'Chaehyun Seo': 5, 'Helene Janicot': 6, 'Jain Kim': 7, 'Janja Garnbret': 8,\n",
    "                       'Jessica Pilz': 9, 'Kyra Condie': 10, 'Laura Rogora': 11, 'Manon Hily': 12,\n",
    "                       'Mia Krampl': 13, 'Miho Nonaka': 14, 'Molly Thompsonsmith': 15,\n",
    "                       'Natalia Grossman': 16, 'Oceania Mackenzie': 17, 'Oriane Bertone': 18,\n",
    "                       'Vita Lukan': 19, 'Yejoo Seo': 20, 'Zelia Avezou': 21}\n",
    "\n",
    "repetition_mapping = {'V1': 1, 'V2': 2, 'V3': 3, 'V4': 4, 'V5': 5, 'V6': 6, 'V7': 7, 'V8': 8, 'V9': 9, 'V10': 10}\n",
    "\n",
    "# Map the categorical columns using the defined mappings\n",
    "data['boulder'] = data['boulder'].map(boulder_mapping)\n",
    "data['camera'] = data['camera'].map(camera_mapping)\n",
    "data['participant'] = data['participant'].map(participant_mapping)\n",
    "data['repetition'] = data['repetition'].map(repetition_mapping)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns=['frame', 'label'])\n",
    "y = data['label']\n",
    "\n",
    "# Split by boulders: W1, W2, W4 for training and W3 for testing\n",
    "train_boulders = [1, 2, 4]  # W1, W2, W4\n",
    "test_boulders = [3]         # W3\n",
    "\n",
    "X_train = X[X['boulder'].isin(train_boulders)]\n",
    "y_train = y[X['boulder'].isin(train_boulders)]\n",
    "X_test = X[X['boulder'].isin(test_boulders)]\n",
    "y_test = y[X['boulder'].isin(test_boulders)]\n",
    "\n",
    "# Ensure the datasets are not empty\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ac81d-c989-4a9a-95b7-e1f3fa796c22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c4320-67eb-48bc-bea1-6e0363ab5da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['boulder'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2262fe-43c1-4765-b5cd-e0d9a6312b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['participant'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a35b7-f2af-4bef-82b1-f7cc6086ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['boulder'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaaebcb-85ba-4f2d-98fe-b67685f15118",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['participant'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31e1fc-899d-4da4-b44c-2bd5f582653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cdb47b-c607-4635-adf4-c3638d7dd7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f80b8d-ba5b-458b-aa5a-791e98abfc4e",
   "metadata": {},
   "source": [
    "### 1.1 - Training the different models: Logistic Regression, Decision Tree, KNN and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d74eb-38f1-4b79-ac86-0f84ac0c8539",
   "metadata": {},
   "source": [
    "Now it could be, that the test set might contain labels that are not present in the training set, since the data was splitted accordingly to the different boulders. The test set contains all frames of boulder W3, and it's possible that there are new movement patterns that were not included in the training set containing boulder W1, W2 and W4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db1be9-660a-4475-a219-036a54811846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    if name == \"Logistic Regression\":\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test_scaled if name == \"Logistic Regression\" else X_test)\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))  # Set zero_division = 0 to avoid the warning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633697e-432b-4bdf-98f4-e0b7e6cfd42a",
   "metadata": {},
   "source": [
    "**The occurrence of many 0.00 values in the precision, recall, and f1-score columns of the classification report might indicate that the corresponding classes have no predicted samples or true samples. This situation can arise for several reasons:**\n",
    "- **Class Imbalance:** If certain classes have very few instances in the dataset compared to others, the model might struggle to make accurate predictions for those classes.\n",
    "\n",
    "- **Data Quality:** Poor data quality, such as missing or noisy data, can affect the model's ability to learn patterns for certain classes.\n",
    "\n",
    "- **Model Complexity:** The chosen model might not be able to effectively capture the underlying patterns in the data, especially for minority classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb0bf53-d171-45ad-b289-1d0c45528c9d",
   "metadata": {},
   "source": [
    "### 1.2 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb51db-074b-447a-ae4c-6aae20eb4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- evaluation plot -------------------------------\n",
    "# Define the metrics for each model manually (replace with your actual results)\n",
    "metrics = {\n",
    "    \"Logistic Regression\": [0.34, 0.32, 0.34, 0.29],\n",
    "    \"Decision Tree\": [0.22, 0.29, 0.22, 0.22],\n",
    "    \"KNN\": [0.33, 0.24, 0.33, 0.26],\n",
    "    \"Random Forest\": [0.37, 0.33, 0.37, 0.30]\n",
    "}\n",
    "\n",
    "# Define the metrics labels\n",
    "metric_labels = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"]\n",
    "\n",
    "# Plot the metrics for each model\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 5))\n",
    "\n",
    "teal_colors = ['#008080', '#009090', '#00A0A0', '#00B0B0']  # Teal color\n",
    "\n",
    "for i, (name, metric_values) in enumerate(metrics.items()):\n",
    "    ax = axes[i]\n",
    "    ax.bar(metric_labels, metric_values, color=teal_colors)\n",
    "    ax.set_title(name)\n",
    "    ax.set_ylim(0, 1)  # Setting y-axis limit to [0, 1] for better visualization\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for name, metric_values in metrics.items():\n",
    "    print(f\"{name}: {metric_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d359dc29-dcb1-4341-b71e-e77b0efbe6d4",
   "metadata": {},
   "source": [
    "**The weighted average was used for the plot**\n",
    "\n",
    "The weighted average takes into account the support (number of samples) for each class when calculating the average, which can be particularly useful when dealing with imbalanced datasets. Since the weighted average gives more weight to classes with larger support, it provides a more representative measure of the overall performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a3aeec-32dc-40bd-8f02-e86756fb8e8e",
   "metadata": {},
   "source": [
    "#### Confusion matrix for the best classifier - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c1592-15e4-4867-8183-3a645c5a9bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=sorted(y_test.unique()), yticklabels=sorted(y_test.unique()))\n",
    "plt.title(f'Confusion Matrix for {name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9588558-8e0d-428d-9e9f-27d3a144674c",
   "metadata": {},
   "source": [
    "## 2 - Classic ML - Using athlete specific data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cf0e73-6716-4320-860f-0c091ba9ed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- some pre-processing -----------------\n",
    "# Define the manual mappings for each categorical column -> boulder, camera, participant, repetition\n",
    "boulder_mapping = {'W1': 1, 'W2': 2, 'W3': 3, 'W4': 4}\n",
    "\n",
    "camera_mapping = {'Cam21': 21, 'Cam22': 22, 'Cam24': 24}\n",
    "\n",
    "participant_mapping = {'Ai Mori': 1, 'Anastasia Sanders': 2, 'Ayala Kerem': 3, 'Brooke Raboutou': 4,\n",
    "                       'Chaehyun Seo': 5, 'Helene Janicot': 6, 'Jain Kim': 7, 'Janja Garnbret': 8,\n",
    "                       'Jessica Pilz': 9, 'Kyra Condie': 10, 'Laura Rogora': 11, 'Manon Hily': 12,\n",
    "                       'Mia Krampl': 13, 'Miho Nonaka': 14, 'Molly Thompsonsmith': 15,\n",
    "                       'Natalia Grossman': 16, 'Oceania Mackenzie': 17, 'Oriane Bertone': 18,\n",
    "                       'Vita Lukan': 19, 'Yejoo Seo': 20, 'Zelia Avezou': 21}\n",
    "\n",
    "repetition_mapping = {'V1': 1, 'V2': 2, 'V3': 3, 'V4': 4, 'V5': 5, 'V6': 6, 'V7': 7, 'V8': 8, 'V9': 9, 'V10': 10}\n",
    "\n",
    "# Map the categorical columns using the defined mappings\n",
    "data['boulder'] = data['boulder'].map(boulder_mapping)\n",
    "data['camera'] = data['camera'].map(camera_mapping)\n",
    "data['participant'] = data['participant'].map(participant_mapping)\n",
    "data['repetition'] = data['repetition'].map(repetition_mapping)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns=['frame', 'label'])\n",
    "y = data['label']\n",
    "\n",
    "# Define the athletes for the test set\n",
    "test_athletes = ['Ai Mori', 'Brooke Raboutou', 'Oceania Mackenzie', 'Mia Krampl']\n",
    "\n",
    "# Get the participant IDs for the test athletes\n",
    "test_athlete_ids = [participant_mapping[athlete] for athlete in test_athletes]\n",
    "\n",
    "# Separate the data into training and test sets based on athletes\n",
    "X_train = X[~(X['participant'].isin(test_athlete_ids))]\n",
    "y_train = y[~(X['participant'].isin(test_athlete_ids))]\n",
    "X_test = X[X['participant'].isin(test_athlete_ids)]\n",
    "y_test = y[X['participant'].isin(test_athlete_ids)]\n",
    "\n",
    "# Ensure the datasets are not empty\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549749eb-c088-4a48-ad63-7bd60a322462",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5432a3-c60f-4b9b-81fb-9fe4c53c6b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['boulder'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf8b92-ef56-435b-9e20-ee1837ef6398",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['participant'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c2146-274f-4a90-8025-cad208165399",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['boulder'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e334f30-92ff-4971-9eb9-e907a392babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['participant'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb9437-47e0-4060-b258-377ec88a96e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9158d551-6dd1-4ea1-94ae-3d7bf9e3d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b868eaf8-c462-4a26-97e6-e62eef4014aa",
   "metadata": {},
   "source": [
    "### 2.1 - Training the different models: Logistic Regression, Decision Tree, KNN and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463dc75e-ba83-46c4-bcfa-d1e3afdb39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    if name == \"Logistic Regression\":\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test_scaled if name == \"Logistic Regression\" else X_test)\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))  # Set zero_division = 0 to avoid the warning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea12700-e4f6-48ec-b5a6-1e08cb53be19",
   "metadata": {},
   "source": [
    "**Interpreting the results:**\n",
    "- **Logistic Regression:** Achieved relatively high precision and recall for some classes like \"no_movement_of_interest\" and \"before_start_position\" but lower performance for others.\n",
    "- **Decision Tree:** Shows varying performance across different classes, with lower precision and recall compared to Logistic Regression.\n",
    "- **KNN:** Generally lower precision, recall, and accuracy compared to other classifiers, indicating weaker performance overall.\n",
    "- **Random Forest:** Achieved the highest accuracy among the classifiers, with relatively high precision and recall for many classes, especially for \"no_movement_of_interest\" and \"before_start_position\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ee7d4-5ae5-4db2-862e-13edf5f41489",
   "metadata": {},
   "source": [
    "### 2.2 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38905ba8-9d44-4f24-bd89-551d78636754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- evaluation plot -------------------------------\n",
    "# Define the metrics for each model manually (replace with your actual results)\n",
    "metrics = {\n",
    "    \"Logistic Regression\": [0.65, 0.63, 0.65, 0.63],\n",
    "    \"Decision Tree\": [0.56, 0.56, 0.56, 0.55],\n",
    "    \"KNN\": [0.40, 0.40, 0.40, 0.40],\n",
    "    \"Random Forest\": [0.72, 0.70, 0.72, 0.70]\n",
    "}\n",
    "\n",
    "# Define the metrics labels\n",
    "metric_labels = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"]\n",
    "\n",
    "# Plot the metrics for each model\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 5))\n",
    "\n",
    "teal_colors = ['#008080', '#009090', '#00A0A0', '#00B0B0']  # Teal color\n",
    "\n",
    "for i, (name, metric_values) in enumerate(metrics.items()):\n",
    "    ax = axes[i]\n",
    "    ax.bar(metric_labels, metric_values, color=teal_colors)\n",
    "    ax.set_title(name)\n",
    "    ax.set_ylim(0, 1)  # Setting y-axis limit to [0, 1] for better visualization\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for name, metric_values in metrics.items():\n",
    "    print(f\"{name}: {metric_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabb2bd0-69a0-4311-93a6-60ebe164aae7",
   "metadata": {},
   "source": [
    "#### Confusion matrix for the best classifier - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b80e08-0984-4e03-beb1-4716544b3aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=sorted(y_test.unique()), yticklabels=sorted(y_test.unique()))\n",
    "plt.title(f'Confusion Matrix for {name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf59ad1-93c2-4d72-a3e3-7ac7e627223c",
   "metadata": {},
   "source": [
    "## 3 - RNN - Using boulder specific data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e0d26-06f2-46b2-ba5f-2d4f1f582484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Bidirectional, Attention, Concatenate, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c9e23-695f-45a9-a6fd-d3689649c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../data/dataframes/labels_and_coordinates_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177a30a-f16f-43b2-9ac6-ae531dcf04ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Normalize keypoint data\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Gather all keypoint columns (all columns with a \"_x\", \"_y\", \"_z\", \"_v\", \"_p\", \"com\" or \"angle\" in their name)\n",
    "keypoint_columns = [col for col in data.columns if '_x' in col or '_y' in col or '_z' in col or '_v' in col or '_p' in col or 'com' in col or 'angle' in col]\n",
    "# Apply fit_transform to keypoint column data only\n",
    "data[keypoint_columns] = scaler.fit_transform(data[keypoint_columns])\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "data = pd.get_dummies(data, columns=['boulder', 'camera', 'participant', 'repetition'])\n",
    "\n",
    "# Encode labels\n",
    "if 'label' in data.columns:\n",
    "    label_encoder = LabelEncoder() \n",
    "    data['label'] = label_encoder.fit_transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c595c-491a-4bdb-af19-18b515c97579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column names to an array\n",
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Print all column names\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c6fbe-0a36-42ac-b630-064d8225db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset based on boulder-specific criteria\n",
    "train_data = data[data['boulder_W3'] == 0]  # Rows where 'boulder_W3' is False\n",
    "test_data = data[data['boulder_W3'] == 1]   # Rows where 'boulder_W3' is True\n",
    "\n",
    "# Print the shapes of the training and testing data\n",
    "print(\"Train Data Shape:\", train_data.shape)\n",
    "print(\"Test Data Shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c1c00-6eee-4da7-a458-f63e37d5896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for reshaping\n",
    "timesteps = 2 \n",
    "total_features = data.drop('label', axis=1).shape[1] \n",
    "\n",
    "# A check to ensure that each sequence fed into the model has a consistent shape\n",
    "if total_features % timesteps != 0:\n",
    "    raise ValueError(f\"Number of total features ({total_features}) is not divisible by defined timesteps ({timesteps}).\")\n",
    "features_per_timestep = total_features // timesteps\n",
    "\n",
    "# Reshape data\n",
    "X_train = train_data.drop('label', axis=1).values.reshape(-1, timesteps, features_per_timestep).astype(np.float32)\n",
    "y_train = train_data['label'].values.astype(np.int32)\n",
    "X_test = test_data.drop('label', axis=1).values.reshape(-1, timesteps, features_per_timestep).astype(np.float32)\n",
    "y_test = test_data['label'].values.astype(np.int32)\n",
    "\n",
    "# Define the TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6426c23-67d6-4802-89c7-b974253972bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(timesteps, features_per_timestep, nr_classes):\n",
    "    \"\"\"\n",
    "    This model contains bidirectional LSTMs and self-attention layers\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(timesteps, features_per_timestep))\n",
    "\n",
    "    x1 = Bidirectional(LSTM(64, return_sequences=True))(inputs)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    attention_layer_1 = Attention()([x1, x1])  \n",
    "\n",
    "    x2 = Bidirectional(LSTM(128, return_sequences=True))(x1)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    attention_layer_2 = Attention()([x2, x2])  \n",
    "\n",
    "    x3 = Bidirectional(LSTM(64, return_sequences=False))(x2)  \n",
    "    x3 = Dropout(0.3)(x3)\n",
    "\n",
    "    concatenated = Concatenate()([Flatten()(attention_layer_1), Flatten()(attention_layer_2), x3])\n",
    "\n",
    "    x_final = Dense(128, activation='relu')(concatenated)\n",
    "    x_final = Dropout(0.3)(x_final)\n",
    "    x_final = Dense(64, activation='relu')(x_final)\n",
    "    outputs = Dense(nr_classes, activation='softmax')(x_final)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaabf8ca-d274-400f-bbf2-570ad50926a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(timesteps, features_per_timestep, len(np.unique(y_train)))\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501edb4-c178-4889-a9a6-0ef153895c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy}, Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe7b3f-4f7d-467f-83e2-0961a056f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "# Predict the labels for the test dataset\n",
    "y_pred = model.predict(test_dataset)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Map numeric labels to their corresponding names\n",
    "y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = np.unique(np.concatenate((y_test_labels, y_pred_labels)))\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Generate and display the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=sorted(unique_labels), yticklabels=sorted(unique_labels))\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7b503c-04f6-4d45-ba2a-454ed69cd85e",
   "metadata": {},
   "source": [
    "## 4 - RNN - Using athlete specific data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa0758-3aca-41b0-802d-1284aede68fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Bidirectional, Attention, Concatenate, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964edcc7-72d0-4ace-b1f0-07ebd82a988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('../../data/dataframes/labels_and_coordinates_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a844937a-14b0-4d63-aa60-7b475a14a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Normalize keypoint data\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Gather all keypoint columns (all columns with a \"_x\", \"_y\", \"_z\", \"_v\", \"_p\", \"com\" or \"angle\" in their name)\n",
    "keypoint_columns = [col for col in data.columns if '_x' in col or '_y' in col or '_z' in col or '_v' in col or '_p' in col or 'com' in col or 'angle' in col]\n",
    "# Apply fit_transform to keypoint column data only\n",
    "data[keypoint_columns] = scaler.fit_transform(data[keypoint_columns])\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "data = pd.get_dummies(data, columns=['boulder', 'camera', 'participant', 'repetition'])\n",
    "\n",
    "# Encode labels\n",
    "if 'label' in data.columns:\n",
    "    label_encoder = LabelEncoder() \n",
    "    data['label'] = label_encoder.fit_transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932a91cb-da4d-4a89-b56e-061b05c96fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column names to an array\n",
    "column_names = data.columns.to_numpy()\n",
    "\n",
    "# Print all column names\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586d0524-b1f1-4bb3-add3-641ce197f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the athletes for the test set\n",
    "test_athletes = [\"participant_Ai Mori\", \"participant_Brooke Raboutou\", \"participant_Oceania Mackenzie\", \"participant_Mia Krampl\"]\n",
    "\n",
    "# Split the dataset based on athlete-specific criteria\n",
    "if any(athlete in data.columns for athlete in test_athletes):\n",
    "    test_data = data[data[test_athletes].any(axis=1)]   # Rows where any test athlete is True\n",
    "    train_data = data[~data[test_athletes].any(axis=1)] # Rows where no test athlete is True\n",
    "else:\n",
    "    raise ValueError(\"Test athlete columns not found in the DataFrame.\")\n",
    "\n",
    "# Print the shapes of the training and testing data\n",
    "print(\"Train Data Shape:\", train_data.shape)\n",
    "print(\"Test Data Shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52141317-23f2-4920-8d58-94b21afa9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for reshaping\n",
    "timesteps = 2 \n",
    "total_features = data.drop('label', axis=1).shape[1] \n",
    "\n",
    "# A check to ensure that each sequence fed into the model has a consistent shape\n",
    "if total_features % timesteps != 0:\n",
    "    raise ValueError(f\"Number of total features ({total_features}) is not divisible by defined timesteps ({timesteps}).\")\n",
    "features_per_timestep = total_features // timesteps\n",
    "\n",
    "# Reshape data\n",
    "X_train = train_data.drop('label', axis=1).values.reshape(-1, timesteps, features_per_timestep).astype(np.float32)\n",
    "y_train = train_data['label'].values.astype(np.int32)\n",
    "X_test = test_data.drop('label', axis=1).values.reshape(-1, timesteps, features_per_timestep).astype(np.float32)\n",
    "y_test = test_data['label'].values.astype(np.int32)\n",
    "\n",
    "# Define the TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a433ee3-dbb1-497a-9100-6d4d396100d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(timesteps, features_per_timestep, nr_classes):\n",
    "    \"\"\"\n",
    "    This model contains bidirectional LSTMs and self-attention layers\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(timesteps, features_per_timestep))\n",
    "\n",
    "    x1 = Bidirectional(LSTM(64, return_sequences=True))(inputs)\n",
    "    x1 = Dropout(0.3)(x1)\n",
    "    attention_layer_1 = Attention()([x1, x1])  \n",
    "\n",
    "    x2 = Bidirectional(LSTM(128, return_sequences=True))(x1)\n",
    "    x2 = Dropout(0.3)(x2)\n",
    "    attention_layer_2 = Attention()([x2, x2])  \n",
    "\n",
    "    x3 = Bidirectional(LSTM(64, return_sequences=False))(x2)  \n",
    "    x3 = Dropout(0.3)(x3)\n",
    "\n",
    "    concatenated = Concatenate()([Flatten()(attention_layer_1), Flatten()(attention_layer_2), x3])\n",
    "\n",
    "    x_final = Dense(128, activation='relu')(concatenated)\n",
    "    x_final = Dropout(0.3)(x_final)\n",
    "    x_final = Dense(64, activation='relu')(x_final)\n",
    "    outputs = Dense(nr_classes, activation='softmax')(x_final)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67b297-b829-4e21-97f2-ecece7581295",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(timesteps, features_per_timestep, len(np.unique(y_train)))\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed144bc9-a6b1-4292-a1f6-1db5ba63d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Accuracy: {test_accuracy}, Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29406b4-4f30-4410-aade-a270715676a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "# Predict the labels for the test dataset\n",
    "y_pred = model.predict(test_dataset)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Map numeric labels to their corresponding names\n",
    "y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "\n",
    "# Get unique labels\n",
    "unique_labels = np.unique(np.concatenate((y_test_labels, y_pred_labels)))\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Generate and display the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=sorted(unique_labels), yticklabels=sorted(unique_labels))\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aecbfa-8b28-4147-b3c6-80d7dd1ee160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# Normalize the confusion matrix by row (true labels)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm_normalized = np.round(cm_normalized, 2)  # Round to 2 decimal places\n",
    "\n",
    "# Display the normalized confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=label_encoder.classes_)\n",
    "fig, ax = plt.subplots(figsize=(14, 14))  # Increase figure size for better readability\n",
    "disp.plot(xticks_rotation='vertical', ax=ax, cmap='viridis')  # Choose a colormap for better contrast\n",
    "\n",
    "# Adjust font size\n",
    "plt.xticks(fontsize=10, ha='center')\n",
    "plt.yticks(fontsize=10, va='center')\n",
    "\n",
    "# Update text properties in the matrix\n",
    "for text in disp.text_.ravel():\n",
    "    text.set_fontsize(10)  # Set font size for the numbers\n",
    "\n",
    "plt.tight_layout(pad=3.0)  # Add padding to ensure elements are not overlapping\n",
    "\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
